# RandomUser Data Engineering Pipeline

## ğŸ“Œ Project Overview

This project implements an end-to-end data pipeline that:

- Ingests user data from the RandomUser API
- Normalizes nested JSON into relational tables
- Loads data into Google BigQuery
- Ensures idempotent re-runs using MERGE logic
- Implements retry handling
- Tracks pipeline execution using system audit tables

The pipeline is built using Python and Google BigQuery and follows production-grade data engineering best practices.

---

## ğŸ— Architecture

```
RandomUser API
        â†“
API Ingestion (Retry Enabled)
        â†“
Transformation (JSON â†’ Normalized Tables)
        â†“
BigQuery Load
    - Staging Table
    - MERGE into Target Tables
        â†“
System Tables (Audit + Metrics)
```

---

## ğŸ“‚ Project Structure

```
randomuser_pipeline/
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml
â”‚
â”œâ”€â”€ credentials/
â”‚   â””â”€â”€ service_account.json
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â””â”€â”€ api_client.py
â”‚   â”œâ”€â”€ transform/
â”‚   â”‚   â””â”€â”€ transformer.py
â”‚   â”œâ”€â”€ load/
â”‚   â”‚   â””â”€â”€ bigquery_loader.py
â”‚   â”œâ”€â”€ system/
â”‚   â”‚   â””â”€â”€ system_tables.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ helpers.py
â”‚   â””â”€â”€ main.py
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ” Authentication Handling

- Uses Google Service Account JSON key.
- Authentication handled via:

```python
service_account.Credentials.from_service_account_file(...)
```

- Access tokens are automatically refreshed by Google SDK.
- No manual token management required.

---

## ğŸŒ API Ingestion

API Used:

```
https://randomuser.me/api/?results=100&seed=easytest
```

Features:
- Retry handling with exponential backoff
- Network failure handling
- 5xx response handling
- Config-driven API parameters

---

## ğŸ§© Schema Design (Normalization)

The API returns deeply nested JSON.  
The schema was normalized into 7 relational tables.

### Core Tables

#### 1ï¸âƒ£ users
- user_id (Primary Key)
- gender
- email
- phone
- cell
- nat
- registered_date

#### 2ï¸âƒ£ names
- user_id (Foreign Key)
- title
- first
- last

#### 3ï¸âƒ£ locations
- user_id
- street_number
- street_name
- city
- state
- country
- postcode
- latitude
- longitude
- timezone_offset
- timezone_description

#### 4ï¸âƒ£ logins
- user_id
- username
- password
- salt
- md5
- sha1
- sha256

#### 5ï¸âƒ£ dobs
- user_id
- dob
- age

#### 6ï¸âƒ£ ids
- user_id
- id_name
- id_value

#### 7ï¸âƒ£ pictures
- user_id
- large
- medium
- thumbnail

### Primary Key Strategy

The `login.uuid` field from the API is used as the primary key (`user_id`) across all tables to maintain referential integrity and ensure idempotency.

---

## ğŸ”„ Idempotency Strategy

To ensure safe re-runs:

1. Data is first loaded into a staging table.
2. A BigQuery MERGE statement is executed:

```sql
MERGE target T
USING staging S
ON T.user_id = S.user_id
WHEN MATCHED THEN UPDATE
WHEN NOT MATCHED THEN INSERT
```

This guarantees:

- No duplicate records
- Safe pipeline re-execution
- Data consistency
- Update capability if data changes

---

## ğŸ“Š System Tables

Three system tables were created:

### metadata_info
Tracks pipeline execution metadata:
- run_id
- dataset_name
- run_timestamp
- status
- records_fetched

### pipeline_audit_logs
Tracks step-level logging:
- run_id
- step_name
- status
- message
- timestamp

### pipeline_metrics
Tracks execution metrics:
- run_id
- records_loaded
- timestamp

System tables use APPEND logic to preserve historical pipeline runs.

---

## ğŸ“ˆ Logging Strategy

The pipeline logs:

- Pipeline start and completion
- API record count
- Per-table merge row counts
- Execution time
- Errors (if any)

Row counts are logged during each MERGE operation to validate idempotency and data consistency.

---

## ğŸš€ How to Run Locally

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/Shaktigupta931/randomuser-data-pipeline.git
cd randomuser-data-pipeline
```

### 2ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Add Service Account Key

Place your Google Cloud service account JSON file inside:

```
credentials/service_account.json
```

âš ï¸ Note: The credentials folder is excluded from Git for security reasons.

### 5ï¸âƒ£ Run the Pipeline

```bash
python3 -m src.main
```

---

## â˜ï¸ Automation Strategy

The pipeline can be automated using the following approaches:

### Option 1: Cloud Run + Cloud Scheduler (Recommended)

1. Containerize the application using Docker.
2. Push the Docker image to Google Artifact Registry.
3. Deploy the container to Cloud Run.
4. Use Cloud Scheduler to trigger execution on a schedule.

Benefits:
- Serverless
- Scalable
- Fully managed
- Minimal operational overhead

---

### Option 2: GitHub Actions + GCP

1. Store repository in GitHub.
2. Create a workflow YAML file.
3. Authenticate using service account secrets.
4. Trigger pipeline on schedule or on push.

Benefits:
- CI/CD integration
- Version control
- Automated deployments

---

## ğŸ¯ Design Highlights

- Modular architecture
- Config-driven pipeline
- Service account authentication
- Retry handling with exponential backoff
- Idempotent MERGE loading strategy
- Normalized relational schema
- System audit and metrics tracking
- Automation-ready design

---

## âœ… Assessment Completion

âœ” API ingestion  
âœ” 7 normalized tables  
âœ” 3 system tables  
âœ” BigQuery loading  
âœ” Retry handling  
âœ” Idempotency  
âœ” Service account authentication  
âœ” Automation explanation  

---

## ğŸ“Œ Conclusion

This project demonstrates production-grade data engineering practices including:

- Clean modular architecture
- Idempotent data loading
- Observability through audit tables
- Secure authentication handling
- Scalable automation-ready design
